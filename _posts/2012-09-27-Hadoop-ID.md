---
layout: post
title: Inside Hadoop: how to submit a MapReduce job?
category : hadoop
tags : [hadoop]
---
{% include JB/setup %}

`Hadoop ID`  
当提交作业到Hadoop时，hadoop首先会生成一个jobId，当启动map task或reduce task时会再生成一个taskId，task会有重试机制即TaskAttempt，会再生成相应的taskAttemptid,
这三者的关系如下：

	ID		// abstract general identifier
	--JobID
	----TaskID
	------TaskAttemptID


#### 1. JobID
JobID由JobTracker的id和job本身的id（顺序号）构成，如`JobID(String jtIdentifier, int id)`。  
第一个参数jtIdentifier如何定义？从`JobTracker.main`方法进入，main方法里调用了`JobTracker.startTracker(JobConf conf)`启动JobTracker进程，
startTracker又调用了`JobTracker.generateNewIdentifier()`，以`yyyyMMddHHmm`格式化当前事件生成JobTracker ID。

	private static SimpleDateFormat getDateFormat() {
    	return new SimpleDateFormat("yyyyMMddHHmm");
  	}
	
  	private static String generateNewIdentifier() {
    	return getDateFormat().format(new Date());
  	}

第二个参数id实际就是job的顺序号，见`JobTracker.getNewJobId()`：

	/**
	* Allocates a new JobId string.
	*/
	public synchronized JobID getNewJobId() throws IOException {
		return new JobID(getTrackerIdentifier(), nextJobId++);
	}

JobID最终使用下划线连接常量"job"、jtIdentifier及id生成作业ID，例如job_201209251153_0921。

#### 2. TaskID
提交作业的流程如下图，最总会调用`JobTracker.submitJob`构造JobInProgress，并添加到JobInProgress监听器里，监听器负责异步初始化作业和任务，最后在构造TaskInProgress时生成TaskID。  
`TaskInProgress.init(JobID jobId)`源码如下：

	/**
	* Initialization common to Map and Reduce
	*/
	void init(JobID jobId) {
		this.startTime = jobtracker.getClock().getTime();
		this.id = new TaskID(jobId, isMapTask(), partition);
		this.skipping = startSkipping();
	}

其中partition即为map/reduce task的顺序号。  
提交作业流程如下图：  
![SubmitJob](https://github.com/gengmzh/gengmzh.github.com/raw/master/_includes/hadoop_submit.jpg)  


#### 3. TaskAttemptID
Hadoop任务节点（TaskTracker）通过heartbeat向主节点（JobTracker）反馈自身运行情况并获取新任务，流程如下图：  
![SubmitJob](https://github.com/gengmzh/gengmzh.github.com/raw/master/_includes/hadoop_heartbeat.jpg)  

最终调用`TaskInProgress.getTaskToRun`生成TaskAttemptID，代码如下：

	TaskAttemptID taskid = null;
	if (nextTaskId < (MAX_TASK_EXECS + maxTaskAttempts + numKilledTasks)) {
		// Make sure that the attempts are unqiue across restarts
		int attemptId = job.getNumRestarts() * NUM_ATTEMPTS_PER_RESTART + nextTaskId;
		taskid = new TaskAttemptID( id, attemptId);
		++nextTaskId;
	}


#### references
+ [MapReduce Tutorial](http://hadoop.apache.org/docs/r1.0.3/mapred_tutorial.html)
